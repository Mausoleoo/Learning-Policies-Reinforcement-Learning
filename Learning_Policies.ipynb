{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a simple 5 × 5 gridworld problem, described below. This is the simplest abstraction of a reinforcement learning problem that allows us to benchmark and compare various learning algorithms to one another and is known as the ‘gridworld’ environment."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 25 cells of the gridworld represent a possible state of the world. An agent in the gridworld environment can take a step up, down, left or right. If the agent attempts to step off the grid, the location of the agent remains unchanged.\n",
    "\n",
    "The blue, green, red and yellow squares represent special states at which the behaviour of the system is as follows. At the blue square, any action yields a reward of 5 and causes the agent to jump to the red square. At the green square, any action yields a reward of 2.5 and causes the agent to jump to either the yellow square or the red square with probability 0.5.\n",
    "\n",
    "An attempt to step off the grid yields a reward of −0.5 and any move from a white square to another white square yields a reward of 0. Intuitively, an agent with a good policy should try to find the states with a high value, and exploit the rewards available at those states.\n",
    "\n",
    "1. Consider a reward discount of γ = 0.95 and a policy which simply moves to one of the four directions with equal probability of 0.25. Estimate the value function for each of the states using (1) solving the system of Bellman equations explicitly (2) iterative policy evaluation (3) value iteration. Which states have the highest value? Does this surprise you?\n",
    "\n",
    "2. Determine the optimal policy for the gridworld problem by (1) explicitly solving the Bellman optimality equation (2) using policy iteration with iterative policy evaluation (3) policy improvement with value iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid_size = 5\n",
    "\n",
    "gridworld = np.zeros((Grid_size, Grid_size)) #Row, Columns\n",
    "\n",
    "#Blue Square\n",
    "gridworld[0][1] = 5 #Jump to Red\n",
    "\n",
    "#Green Square\n",
    "gridworld[0][4] = 2.5 #Jump to Yellow or Red with probability 0.5\n",
    "\n",
    "step = np.array([[-1,0],[1,0],[0,-1],[0,1]]) #Up, #Down, #Left, #Right\n",
    "\n",
    "Yellow_Red = np.array([[4,4],[3,2]])\n",
    "\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reward_And_Transition(Current_Step, Action):\n",
    "\n",
    "    if Current_Step == [0,1]:\n",
    "        Next_Step = [3,2]\n",
    "        Reward = 5\n",
    "        return  Next_Step, Reward\n",
    "    \n",
    "    elif Current_Step == [0,4]:\n",
    "        Next_Step = Yellow_Red[np.random.randint(2)]\n",
    "        Reward = 2.5\n",
    "        return  Next_Step, Reward\n",
    "\n",
    "    Next_Step = Current_Step + Action\n",
    "\n",
    "    if Next_Step[0] < 0 or Next_Step[0] > 4 or Next_Step[1] < 0 or Next_Step[1] > 4:\n",
    "        Reward = -0.5\n",
    "        return Current_Step, Reward\n",
    "    Reward = 0\n",
    "\n",
    "    return Next_Step, Reward   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Value_Policy = np.zeros((Grid_size, Grid_size))\n",
    "threshold = 1e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "while 1:\n",
    "    delta = 0\n",
    "    for i in range(Grid_size):\n",
    "        for j in range(Grid_size):\n",
    "            Value_Policy_Current = Value_Policy[i,j]\n",
    "            aux = 0\n",
    "            #There are 4 directions            \n",
    "            for k in range(step.shape[0]): \n",
    "                New_Step, Reward = Reward_And_Transition([i,j], step[k])\n",
    "                aux += 0.25 * (Reward + gamma * Value_Policy[New_Step[0], New_Step[1]])\n",
    "            Value_Policy[i,j] = aux\n",
    "            delta = max(delta, abs(Value_Policy_Current - aux))\n",
    "    if delta < threshold:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Value_Policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
